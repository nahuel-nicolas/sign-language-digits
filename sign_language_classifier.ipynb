{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Digits Classifier\n",
    "Using MobileNet transfer learning to classify ASL digits (0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Project Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project constants\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "VALIDATION_SPLIT = 0.2\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = 'Sign-Language-Digits-Dataset/Dataset'\n",
    "SAMPLE_PATH = 'Sign-Language-Digits-Dataset/sample'\n",
    "EVAL_PATH = 'Sign-Language-Digits-Dataset/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset Splits\n",
    "Creating sample/ folder (250 images for training) and eval/ folder (250 images for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sample/ folder...\n",
      "Created 250 images in sample/ folder\n",
      "\n",
      "Creating eval/ folder...\n",
      "Created 250 images in eval/ folder\n"
     ]
    }
   ],
   "source": [
    "# Create sample/ and eval/ folders with random images\n",
    "def create_dataset_split(source_path, dest_path, num_images_per_class=25, exclude_images=None):\n",
    "    \"\"\"Create a dataset folder with random images from source\"\"\"\n",
    "    if exclude_images is None:\n",
    "        exclude_images = set()\n",
    "    \n",
    "    # Remove destination if it exists\n",
    "    if os.path.exists(dest_path):\n",
    "        shutil.rmtree(dest_path)\n",
    "    os.makedirs(dest_path)\n",
    "    \n",
    "    selected_images = set()\n",
    "    \n",
    "    # Process each digit (0-9)\n",
    "    for digit in range(10):\n",
    "        digit_source = os.path.join(source_path, str(digit))\n",
    "        digit_dest = os.path.join(dest_path, str(digit))\n",
    "        os.makedirs(digit_dest)\n",
    "        \n",
    "        # Get all images for this digit\n",
    "        all_images = [f for f in os.listdir(digit_source) if f.endswith('.JPG')]\n",
    "        \n",
    "        # Filter out excluded images\n",
    "        available_images = [img for img in all_images \n",
    "                          if os.path.join(str(digit), img) not in exclude_images]\n",
    "        \n",
    "        # Randomly select images\n",
    "        selected = random.sample(available_images, num_images_per_class)\n",
    "        \n",
    "        # Copy selected images\n",
    "        for img in selected:\n",
    "            src = os.path.join(digit_source, img)\n",
    "            dst = os.path.join(digit_dest, img)\n",
    "            shutil.copy2(src, dst)\n",
    "            selected_images.add(os.path.join(str(digit), img))\n",
    "    \n",
    "    return selected_images\n",
    "\n",
    "# Create sample/ folder (25 images per digit)\n",
    "print('Creating sample/ folder...')\n",
    "sample_images = create_dataset_split(DATASET_PATH, SAMPLE_PATH, num_images_per_class=25)\n",
    "print(f'Created {len(sample_images)} images in sample/ folder')\n",
    "\n",
    "# Create eval/ folder (25 different images per digit)\n",
    "print('\\nCreating eval/ folder...')\n",
    "eval_images = create_dataset_split(DATASET_PATH, EVAL_PATH, num_images_per_class=25, \n",
    "                                   exclude_images=sample_images)\n",
    "print(f'Created {len(eval_images)} images in eval/ folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model Architecture\n",
    "Using MobileNetV2 as base model with frozen weights, adding custom classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileNetV2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37738/1689098958.py:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: block_16_depthwise_BN\n",
      "  Type: BatchNormalization\n",
      "  Output Shape: (None, None, None, 960)\n",
      "  Params: 3840\n",
      "--------------------------------------------------\n",
      "Layer: block_16_depthwise_relu\n",
      "  Type: ReLU\n",
      "  Output Shape: (None, None, None, 960)\n",
      "  Params: 0\n",
      "--------------------------------------------------\n",
      "Layer: block_16_project\n",
      "  Type: Conv2D\n",
      "  Output Shape: (None, None, None, 320)\n",
      "  Params: 307200\n",
      "--------------------------------------------------\n",
      "Layer: block_16_project_BN\n",
      "  Type: BatchNormalization\n",
      "  Output Shape: (None, None, None, 320)\n",
      "  Params: 1280\n",
      "--------------------------------------------------\n",
      "Layer: Conv_1\n",
      "  Type: Conv2D\n",
      "  Output Shape: (None, None, None, 1280)\n",
      "  Params: 409600\n",
      "--------------------------------------------------\n",
      "Layer: Conv_1_bn\n",
      "  Type: BatchNormalization\n",
      "  Output Shape: (None, None, None, 1280)\n",
      "  Params: 5120\n",
      "--------------------------------------------------\n",
      "Layer: out_relu\n",
      "  Type: ReLU\n",
      "  Output Shape: (None, None, None, 1280)\n",
      "  Params: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import MobileNet and create model\n",
    "print('Loading MobileNetV2...')\n",
    "base_model = MobileNetV2(\n",
    "    # input_shape=(100, 100, 3), \n",
    "    include_top=False, \n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze base model layers\n",
    "# base_model.trainable = False\n",
    "\n",
    "# Get last 7 layers with more details\n",
    "for layer in base_model.layers[-7:]:\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Type: {layer.__class__.__name__}\")\n",
    "    print(f\"  Output Shape: {layer.output.shape}\")\n",
    "    print(f\"  Params: {layer.count_params()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture:\n",
      "Layer: block_16_project_BN\n",
      "  Type: BatchNormalization\n",
      "  Output Shape: (None, None, None, 320)\n",
      "  Params: 1280\n",
      " Trainable: True\n",
      "--------------------------------------------------\n",
      "Layer: Conv_1\n",
      "  Type: Conv2D\n",
      "  Output Shape: (None, None, None, 1280)\n",
      "  Params: 409600\n",
      " Trainable: True\n",
      "--------------------------------------------------\n",
      "Layer: Conv_1_bn\n",
      "  Type: BatchNormalization\n",
      "  Output Shape: (None, None, None, 1280)\n",
      "  Params: 5120\n",
      " Trainable: True\n",
      "--------------------------------------------------\n",
      "Layer: out_relu\n",
      "  Type: ReLU\n",
      "  Output Shape: (None, None, None, 1280)\n",
      "  Params: 0\n",
      " Trainable: True\n",
      "--------------------------------------------------\n",
      "Layer: global_average_pooling2d_55\n",
      "  Type: GlobalAveragePooling2D\n",
      "  Output Shape: (None, 1280)\n",
      "  Params: 0\n",
      " Trainable: True\n",
      "--------------------------------------------------\n",
      "Layer: dropout_14\n",
      "  Type: Dropout\n",
      "  Output Shape: (None, 1280)\n",
      "  Params: 0\n",
      " Trainable: True\n",
      "--------------------------------------------------\n",
      "Layer: dense_43\n",
      "  Type: Dense\n",
      "  Output Shape: (None, 10)\n",
      "  Params: 12810\n",
      " Trainable: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build model on top of MobileNet\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "predictions = Dense(\n",
    "    NUM_CLASSES,\n",
    "    activation='softmax',\n",
    "    # kernel_regularizer=regularizers.l2(0.1)\n",
    ")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print('\\nModel architecture:')\n",
    "# Get last 7 layers with more details\n",
    "for layer in model.layers[-7:]:\n",
    "    print(f\"Layer: {layer.name}\")\n",
    "    print(f\"  Type: {layer.__class__.__name__}\")\n",
    "    print(f\"  Output Shape: {layer.output.shape}\")\n",
    "    print(f\"  Params: {layer.count_params()}\")\n",
    "    print(f\" Trainable: {layer.trainable}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Data Generators\n",
    "Setting up training and validation generators with 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1653 images belonging to 10 classes.\n",
      "Found 409 images belonging to 10 classes.\n",
      "\n",
      "Training samples: 1653\n",
      "Validation samples: 409\n"
     ]
    }
   ],
   "source": [
    "# Create image data generators with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    # Data augmentation parameters\n",
    "    rotation_range=15,              # Randomly rotate images by up to 15 degrees\n",
    "    zoom_range=0.15,                # Randomly zoom in/out by up to 15%\n",
    "    brightness_range=[0.8, 1.2],    # Randomly adjust brightness\n",
    "    fill_mode='nearest'             # Fill pixels after transformations\n",
    ")\n",
    "\n",
    "# Validation generator (no augmentation, only rescaling)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=VALIDATION_SPLIT\n",
    ")\n",
    "\n",
    "# Training generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Validation generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f'\\nTraining samples: {train_generator.samples}')\n",
    "print(f'Validation samples: {validation_generator.samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 649ms/step - accuracy: 0.4870 - loss: 1.5191 - val_accuracy: 0.5819 - val_loss: 1.2674\n",
      "Epoch 2/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 662ms/step - accuracy: 0.7792 - loss: 0.7021 - val_accuracy: 0.6039 - val_loss: 1.0714\n",
      "Epoch 3/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 663ms/step - accuracy: 0.8433 - loss: 0.5092 - val_accuracy: 0.6528 - val_loss: 0.9570\n",
      "Epoch 4/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 627ms/step - accuracy: 0.8730 - loss: 0.4281 - val_accuracy: 0.6748 - val_loss: 0.9081\n",
      "Epoch 5/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 607ms/step - accuracy: 0.8857 - loss: 0.3717 - val_accuracy: 0.6968 - val_loss: 0.8342\n",
      "Epoch 6/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 617ms/step - accuracy: 0.9093 - loss: 0.3197 - val_accuracy: 0.7311 - val_loss: 0.7576\n",
      "Epoch 7/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 611ms/step - accuracy: 0.9189 - loss: 0.2815 - val_accuracy: 0.7335 - val_loss: 0.7255\n",
      "Epoch 8/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 600ms/step - accuracy: 0.9195 - loss: 0.2588 - val_accuracy: 0.7775 - val_loss: 0.6530\n",
      "Epoch 9/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 597ms/step - accuracy: 0.9347 - loss: 0.2331 - val_accuracy: 0.7555 - val_loss: 0.6597\n",
      "Epoch 10/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 621ms/step - accuracy: 0.9401 - loss: 0.2147 - val_accuracy: 0.7628 - val_loss: 0.6427\n",
      "Epoch 11/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 612ms/step - accuracy: 0.9492 - loss: 0.1858 - val_accuracy: 0.7800 - val_loss: 0.6149\n",
      "Epoch 12/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 620ms/step - accuracy: 0.9522 - loss: 0.1874 - val_accuracy: 0.7897 - val_loss: 0.5803\n",
      "Epoch 13/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 625ms/step - accuracy: 0.9577 - loss: 0.1727 - val_accuracy: 0.7873 - val_loss: 0.5659\n",
      "Epoch 14/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 640ms/step - accuracy: 0.9534 - loss: 0.1735 - val_accuracy: 0.7971 - val_loss: 0.5455\n",
      "Epoch 15/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 604ms/step - accuracy: 0.9601 - loss: 0.1460 - val_accuracy: 0.8142 - val_loss: 0.5151\n",
      "Epoch 16/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 615ms/step - accuracy: 0.9577 - loss: 0.1446 - val_accuracy: 0.8313 - val_loss: 0.4931\n",
      "Epoch 17/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 634ms/step - accuracy: 0.9577 - loss: 0.1458 - val_accuracy: 0.8289 - val_loss: 0.4911\n",
      "Epoch 18/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 605ms/step - accuracy: 0.9704 - loss: 0.1221 - val_accuracy: 0.8411 - val_loss: 0.4703\n",
      "Epoch 19/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 635ms/step - accuracy: 0.9691 - loss: 0.1259 - val_accuracy: 0.8411 - val_loss: 0.4781\n",
      "Epoch 20/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 674ms/step - accuracy: 0.9716 - loss: 0.1124 - val_accuracy: 0.8411 - val_loss: 0.4556\n",
      "Epoch 21/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 647ms/step - accuracy: 0.9637 - loss: 0.1230 - val_accuracy: 0.8386 - val_loss: 0.4576\n",
      "Epoch 22/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 667ms/step - accuracy: 0.9698 - loss: 0.1116 - val_accuracy: 0.8509 - val_loss: 0.4210\n",
      "Epoch 23/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 614ms/step - accuracy: 0.9740 - loss: 0.1023 - val_accuracy: 0.8533 - val_loss: 0.4220\n",
      "Epoch 24/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 604ms/step - accuracy: 0.9740 - loss: 0.1058 - val_accuracy: 0.8460 - val_loss: 0.4235\n",
      "Epoch 25/25\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 602ms/step - accuracy: 0.9776 - loss: 0.0964 - val_accuracy: 0.8582 - val_loss: 0.4187\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print('Training model...')\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model\n",
    "Testing on unseen eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on eval/ folder\n",
    "eval_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "eval_generator = eval_datagen.flow_from_directory(\n",
    "    EVAL_PATH,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print('Evaluating model on eval dataset...')\n",
    "eval_loss, eval_accuracy = model.evaluate(eval_generator)\n",
    "print(f'\\nEval Loss: {eval_loss:.4f}')\n",
    "print(f'Eval Accuracy: {eval_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for confusion matrix\n",
    "eval_generator.reset()\n",
    "predictions = model.predict(eval_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = eval_generator.classes\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title('Confusion Matrix - Sign Language Digits')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(true_classes, predicted_classes, \n",
    "                          target_names=[str(i) for i in range(10)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
